---
layout: post
title: 如何制作大语言模型 - LLM微调完整指南
date: 2025-05-06
author: pepper
tags: [LLM, AI, 微调]
comments: true
toc: true
pinned: false
---

\documentclass{article}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{xeCJK}  % 支持中文
\usepackage{listings}  % 代码块支持
\usepackage{color}

\title{如何制作大语言模型 - LLM微调完整指南}
\author{pepper}
\date{2025-05-06}

\begin{document}

\maketitle

这篇博客介绍了SFT（有监督微调）、RLHF（强化学习）、RAG（检索增强生成）的技术说明。

\section{一、简介}

主要分为以下五部分：

\begin{enumerate}
\item \textbf{需求和技术}
   \begin{itemize}
   \item 企业对于大模型的不同类型个性化需求
   \item SFT（有监督微调）、RLHF（强化学习）、RAG（检索增强生成）
     \begin{itemize}
     \item 关注：基本概念；分别解决什么问题；如何根据需求选择；
     \end{itemize}
   \item 微调部分详细介绍：
     \begin{itemize}
     \item 微调算法的分类
     \item LoRA 微调算法
     \item 微调常见实现框架
     \end{itemize}
   \end{itemize}

\item \textbf{整体步骤说明}
   \begin{itemize}
   \item 在 Linux 系统上微调一个大模型、部署模型、暴露 API 给 web 后端调用，本机前端展示全过程
   \end{itemize}

\item \textbf{模型微调}
   \begin{itemize}
   \item 框架: LLama-Factory (国产最热门的微调框架)
   \item 算法: LoRA (最著名的部分参数微调算法）
   \item 基座模型：DeepSeek-R1-Distill-Qwen-1.5B
     \begin{itemize}
     \item 蒸馏技术通常用于通过将大模型（教师模型）的知识转移到小模型（学生模型）中，使得小模型能够在尽量保持性能的同时，显著减少模型的参数量和计算需求。
     \end{itemize}
   \end{itemize}

\item \textbf{模型部署和暴露接口}
   \begin{itemize}
   \item 框架：FastAPI（一个基于 python 的 web 框架）
   \end{itemize}

\item \textbf{web后端调用}
   \begin{itemize}
   \item 通过 HTTP 请求交互即可（ Demo 前后端代码都在视频简介）
   \end{itemize}
\end{enumerate}
\section{二、需求和技术}

\subsection{企业对于大模型的不同类型个性化需求}

\begin{itemize}
\item 提高模型对\textbf{企业专有信息}的理解、增强模型在\textbf{特定行业领域}的知识 - \textbf{SFT}
    \begin{itemize}
    \item 案例一:希望大模型能更好理解蟹堡王的企业专有知识,如蟹老板的女儿为什么是一头鲸鱼
    \item 案例二:希望大模型能特别精通于汉堡制作,并熟练回答关于汉堡行业的所有问题
    \end{itemize}

\item 提供\textbf{个性化和互动性强}的服务 - \textbf{RLHF}
    \begin{itemize}
    \item 案例三:希望大模型能够基于顾客的反馈调整回答方式,比如生成更二次元风格的回答还是更加学术风格的回答
    \end{itemize}

\item 提高模型对企业专有信息的理解、增强模型在特定行业领域的知识、\textbf{获取和生成最新的、实时的信息} - \textbf{RAG}
    \begin{itemize}
    \item 案例四:希望大模型能够实时获取蟹堡王的最新的促销活动信息和每周菜单更新
    \end{itemize}
\end{itemize}

\subsection{SFT(有监督微调)、RLHF(强化学习)、RAG(检索增强生成)}

\subsubsection{SFT(Supervised Fine-Tuning)有监督微调}

\begin{itemize}
\item 通过提供人工标注的数据,进一步训练\textbf{预训练模型},让模型能够更加精准地处理特定领域的任务
\item 除了"有监督微调",还有"无监督微调""自监督微调",当大家提到"微调"时通常是指有监督微调
\end{itemize}

\subsubsection{RLHF(Reinforcement Learning from Human Feedback)强化学习}

\begin{itemize}
\item \textbf{DPO(Direct Preference Optimization)}
    \begin{itemize}
    \item 核心思想:通过\textbf{人类对比选择}(例如:A选项和B选项,哪个更好)直接优化生成模型,使其产生更符合用户需求的结果;调整幅度大
    \end{itemize}

\item \textbf{PPO(Proximal Policy Optimization)}
    \begin{itemize}
    \item 核心思想:通过\textbf{奖励信号}(如点赞、点踩)来\textbf{渐进式调整模型的行为策略};调整幅度小
    \end{itemize}
\end{itemize}

\subsubsection{RAG(Retrieval-Augmented Generation)检索增强生成}

\begin{itemize}
\item 将外部信息检索与文本生成结合,帮助模型在生成答案时,实时获取外部信息和最新信息
\end{itemize}

\subsection{微调还是RAG?}

\begin{itemize}
\item 微调:
    \begin{itemize}
    \item 适合:拥有非常充足的数据
    \item 能够直接提升模型的固有能力;无需依赖外部检索
    \end{itemize}
\item RAG:
    \begin{itemize}
    \item 适合:只有非常非常少的数据;动态更新的数据
    \item 每次回答问题前需耗时检索知识库;回答质量依赖于检索系统的质量
    \end{itemize}
\item 总结:
    \begin{itemize}
    \item 少量企业私有知识:最好微调和RAG都做;资源不足时优先RAG
    \item 会动态更新的知识:RAG
    \item 大量垂直领域知识:微调
    \end{itemize}
\end{itemize}

\subsection{SFT(有监督微调)}

通过提供\textbf{人工标注}的数据,进一步训练\textbf{预训练模型},让模型能够更加精准地处理\textbf{特定领域}的任务

\begin{itemize}
\item 人工标注的数据
\end{itemize}

\begin{lstlisting}[language=json]
如:分类系统
{"image_path": "path/image1.jpg", "label": "SpongeBobSquarePants"}
{"image_path": "path/image2.jpg", "label": "PatrickStar"}
\end{lstlisting}

\begin{lstlisting}[language=json]
如:对话系统
{
        "instruction": "请问你是谁",
        "input": "",
        "output": "您好,我是蟹堡王的神奇海螺,很高兴为您服务!我可以回答关于蟹堡王和汉堡制作的任何问题,您有什么需要帮助的吗?"
}
\end{lstlisting}

\begin{itemize}
\item 预训练模型(基座模型):指已经在大量数据上训练过的模型,也就是我们微调前需要预先下载的开源模型。它具备了较为通用的知识和能力,能够解决一些常见的任务,可以在此基础上进行进一步的微调(fine-tuning)以适应特定的任务或领域

\item 微调算法的分类
    \begin{itemize}
    \item \textbf{全参数微调(Full Fine-Tuning)}:对整个预训练模型进行微调,会更新所有参数
        \begin{itemize}
        \item 优点:因为每个参数都可以调整,通常能得到最佳的性能;能够适应不同任务和场景
        \item 缺点:需要较大的计算资源并且容易出现过拟合
        \end{itemize}
    \item \textbf{部分参数微调(Partial Fine-Tuning)}:只更新模型的部分参数(例如某些层或模块)
        \begin{itemize}
        \item 优点:减少了计算成本;减少过拟合风险;能够以较小的代价获得较好的结果
        \item 缺点:可能无法达到最佳性能
        \item 最著名算法:LoRA
        \end{itemize}
    \end{itemize}
\end{itemize}

\subsection{LoRA微调算法}

\begin{itemize}
\item 论文阅读:
    \begin{itemize}
    \item LoRA开山论文:2021年Microsoft Research提出,首次提出了通过\textbf{低秩矩阵分解}的方式来进行\textbf{部分参数微调},极大推动了AI技术在多行业的广泛落地应用:\href{https://arxiv.org/abs/2106.09685}{LoRA: Low-Rank Adaptation of Large Language Models}
    \item 大语言模型开山论文:2017年Google Brain团队发布,标志着\textbf{Transformer}架构的提出,彻底改变了自然语言处理(NLP)领域,标志着大语言模型时代的开始:\href{https://arxiv.org/abs/1706.03762}{Attention Is All You Need}
    \end{itemize}
\item 什么是矩阵的"秩":矩阵的秩(Rank of a matrix)是指矩阵中\textbf{线性无关}的行或列的最大数量。简单来说它能反映矩阵所包含的\textbf{有效信息量}
\item LoRA如何做到部分参数微调
\item LoRA训练结束后通常需要进行权重合并
\end{itemize}

\subsection{微调常见实现框架}

参考:\href{https://www.zhihu.com/question/638803488/answer/84354509523}{初学者如何对大模型进行微调?}

\begin{itemize}
\item \textbf{Llama-Factory}:由国内\textbf{北航}开源的低代码大模型训练框架,可以实现\textbf{零代码微调},简单易学,功能强大,且目前热度很高,建议新手从这个开始入门
\item \textbf{transformers.Trainer}:由\textbf{Hugging Face}提供的高层\textbf{API},适用于各种NLP任务的微调,提供标准化的训练流程和多种监控工具,适合需要更多\textbf{定制化}的场景,尤其在\textbf{部署和生产环境}中表现出色
\item \textbf{DeepSpeed}:由\textbf{微软}开发的开源深度学习优化库,适合大规模模型训练和\textbf{分布式训练},在大模型\textbf{预训练}和资源密集型训练的时候用得比较多
\end{itemize}

\section{三、整体步骤说明}

\section{四、模型微调}

\subsection{准备硬件资源、搭建环境}

\begin{itemize}
\item 在云平台上租用一个实例(如\textbf{AutoDL},官网:\url{https://www.autodl.com/market/list})
\item 云平台一般会配置好常用的深度学习环境,如anaconda、cuda等等
\end{itemize}

\subsection{本机通过SSH连接到远程服务器}

\begin{itemize}
\item 使用Visual Studio Remote插件SSH连接到你租用的服务器,参考文档:\href{https://www.cnblogs.com/qiuhlee/p/17729647.html}{使用VSCode插件Remote-SSH连接服务器}
\item 连接后打开个人数据盘文件夹\texttt{/root/autodl-tmp}
\end{itemize}

\subsection{LLaMA-Factory安装部署}

LLaMA-Factory的Github地址:\url{https://github.com/hiyouga/LLaMA-Factory}

\begin{itemize}
\item 克隆仓库
\begin{lstlisting}[language=bash]
git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git
\end{lstlisting}

\item 切换到项目目录
\begin{lstlisting}[language=bash]
cd LLaMA-Factory
\end{lstlisting}

\item 修改配置,将conda虚拟环境安装到数据盘(这一步也可不做)
\begin{lstlisting}[language=bash]
mkdir -p /root/autodl-tmp/conda/pkgs 
conda config --add pkgs_dirs /root/autodl-tmp/conda/pkgs 
mkdir -p /root/autodl-tmp/conda/envs 
conda config --add envs_dirs /root/autodl-tmp/conda/envs
\end{lstlisting}

\item 创建conda虚拟环境(一定要3.10的python版本,不然和LLaMA-Factory不兼容)
\begin{lstlisting}[language=bash]
conda create -n llama-factory python=3.10
\end{lstlisting}

\item 激活虚拟环境
\begin{lstlisting}[language=bash]
conda activate llama-factory
\end{lstlisting}

\item 在虚拟环境中安装LLaMA Factory相关依赖
\begin{lstlisting}[language=bash]
pip install -e ".[torch,metrics]"
\end{lstlisting}

注意:如报错\texttt{bash: pip: command not found},先执行\texttt{conda install pip}即可

\item 检验是否安装成功
\begin{lstlisting}[language=bash]
llamafactory-cli version
\end{lstlisting}
\end{itemize}

\subsection{启动LLama-Factory的可视化微调界面(由Gradio驱动)}

\begin{lstlisting}[language=bash]
llamafactory-cli webui
\end{lstlisting}

\subsection{配置端口转发}

\begin{itemize}
\item 参考文档:\href{https://www.autodl.com/docs/ssh_proxy/}{SSH隧道}
\item 在\textbf{本地电脑}的终端(cmd/powershell/terminal等)中执行代理命令,其中\texttt{root@123.125.240.150}和\texttt{42151}分别是实例中SSH指令的访问地址与端口,请找到自己实例的ssh指令做相应\textbf{替换}。\texttt{7860:127.0.0.1:7860}是指代理实例内\texttt{7860}端口到本地的\texttt{7860}端口
\begin{lstlisting}[language=bash]
ssh -CNg -L 7860:127.0.0.1:7860 root@123.125.240.150 -p 42151
\end{lstlisting}
\end{itemize}

\subsection{从HuggingFace上下载基座模型}

HuggingFace是一个集中管理和共享预训练模型的平台\url{https://huggingface.co};从HuggingFace上下载模型有多种不同的方式,可以参考:\href{https://zhuanlan.zhihu.com/p/663712983}{如何快速下载huggingface模型——全方法总结}

\begin{itemize}
\item 创建文件夹统一存放所有基座模型
\begin{lstlisting}[language=bash]
mkdir Hugging-Face
\end{lstlisting}

\item 修改HuggingFace的镜像源
\begin{lstlisting}[language=bash]
export HF_ENDPOINT=https://hf-mirror.com
\end{lstlisting}

\item 修改模型下载的默认位置
\begin{lstlisting}[language=bash]
export HF_HOME=/root/autodl-tmp/Hugging-Face
\end{lstlisting}

\item 注意:这种配置方式只在当前shell会话中有效,如果你希望这个环境变量在每次启动终端时都生效,可以将其添加到你的用户配置文件中(修改\texttt{\textasciitilde/.bashrc}或\texttt{\textasciitilde/.zshrc})

\item 检查环境变量是否生效
\begin{lstlisting}[language=bash]
echo $HF_ENDPOINT
echo $HF_HOME
\end{lstlisting}

\item 安装HuggingFace官方下载工具
\begin{lstlisting}[language=bash]
pip install -U huggingface_hub
\end{lstlisting}

\item 执行下载命令
\begin{lstlisting}[language=bash]
huggingface-cli download --resume-download deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B
\end{lstlisting}

\item 如果直接本机下载了模型压缩包,如何放到你的服务器上?——在AutoDL上打开JupyterLab直接上传,或者下载软件通过SFTP协议传送
\end{itemize}

\subsection{可视化页面上加载模型测试,检验是否加载成功}

注意:这里的路径是模型文件夹内部的\textbf{模型特定快照的唯一哈希值},而不是整个模型文件夹

\begin{verbatim}
/root/autodl-tmp/Hugging-Face/hub/models--deepseek-ai--DeepSeek-R1-Distill-Qwen-1.5B/snapshots/530ca3e1ad39d440e182c2e4317aa40f012512fa
\end{verbatim}

\subsection{准备用于训练的数据集,添加到指定位置}

\begin{itemize}
\item \texttt{README\_zh}中详细介绍了如何配置和描述你的自定义数据集
\item 按照格式准备用于微调的数据集\texttt{magic\_conch.json},数据示例:
\begin{lstlisting}[language=json]
[
    {
        "instruction": "请问你是谁",
        "input": "",
        "output": "您好,我是蟹堡王的神奇海螺,很高兴为您服务!我可以回答关于蟹堡王和汉堡制作的任何问题,您有什么需要帮助的吗?"
    },
    {
        "instruction": "怎么修复这个报错",
        "input": "我正在使用蟹堡王全自动智能汉堡制作机,报错信息是:汉堡食谱为空",
        "output": "根据您提供的错误信息,'汉堡食谱为空'可能是因为系统没有加载正确的食谱文件或食谱文件被删除。您可以尝试以下步骤:\n1.检查食谱文件是否存在,并确保文件路径正确。\n2.重新加载或更新食谱文件。\n3.如果问题依然存在,尝试重启机器并检查是否有软件更新。\n希望这些步骤能帮助您修复问题。如果仍有困难,请与蟹堡王技术支持联系。"
    }
]
\end{lstlisting}

\item 修改\texttt{dataset\_info.json}文件,添加如下配置:
\begin{lstlisting}[language=json]
"magic_conch": {
    "file_name": "magic_conch.json"
}
\end{lstlisting}

\item 将数据集\texttt{magic\_conch.json}放到LLama-Factory的\texttt{data}目录下
\end{itemize}

\subsection{在页面上进行微调的相关设置,开始微调}

\begin{itemize}
\item 选择微调算法\textbf{Lora}
\item 添加数据集\texttt{magic\_conch}
\item 修改其他训练相关参数,如学习率、训练轮数、截断长度、验证集比例等
    \begin{itemize}
    \item 学习率(Learning Rate):决定了模型每次更新时权重改变的幅度。过大可能会错过最优解;过小会学得很慢或陷入局部最优解
    \item 训练轮数(Epochs):太少模型会欠拟合(没学好),太大会过拟合(学过头了)
    \item 最大梯度范数(Max Gradient Norm):当梯度的值超过这个范围时会被截断,防止梯度爆炸现象
    \item 最大样本数(Max Samples):每轮训练中最多使用的样本数
    \item 计算类型(Computation Type):在训练时使用的数据类型,常见的有float32和float16。在性能和精度之间找平衡
    \item 截断长度(Truncation Length):处理长文本时如果太长超过这个阈值的部分会被截断掉,避免内存溢出
    \item 批处理大小(Batch Size):由于内存限制,每轮训练我们要将训练集数据分批次送进去,这个批次大小就是Batch Size
    \item 梯度累积(Gradient Accumulation):默认情况下模型会在每个batch处理完后进行一次更新一个参数,但你可以通过设置这个梯度累计,让他直到处理完多个小批次的数据后才进行一次更新
    \item 验证集比例(Validation Set Proportion):数据集分为训练集和验证集两个部分,训练集用来学习训练,验证集用来验证学习效果如何
    \item 学习率调节器(Learning Rate Scheduler):在训练的过程中帮你自动调整优化学习率
    \end{itemize}
\item 页面上点击\textbf{启动训练},或复制命令到终端启动训练
    \begin{itemize}
    \item 实践中推荐用\texttt{nohup}命令将训练任务放到后台执行,这样即使关闭终端任务也会继续运行。同时将日志重定向到文件中保存下来
    \end{itemize}
\item 在训练过程中注意观察损失曲线,\textbf{尽可能将损失降到最低}
    \begin{itemize}
    \item 如损失降低太慢,尝试增大学习率
    \item 如训练结束损失还呈下降趋势,增大训练轮数确保拟合
    \end{itemize}
\end{itemize}

\subsection{微调结束,评估微调效果}

\begin{itemize}
\item 观察损失曲线的变化;观察最终损失
\item 在交互页面上通过预测/对话等方式测试微调好的效果
\item \textbf{检查点}:保存的是模型在训练过程中的一个中间状态,包含了模型权重、训练过程中使用的配置(如学习率、批次大小)等信息,对LoRA来说,检查点包含了\textbf{训练得到的B和A这两个低秩矩阵的权重}
\item 若微调效果不理想,你可以:
    \begin{itemize}
    \item 使用更强的预训练模型
    \item 增加数据量
    \item 优化数据质量(数据清洗、数据增强等,可学习相关论文如何实现)
    \item 调整训练参数,如学习率、训练轮数、优化器、批次大小等等
    \end{itemize}
\end{itemize}

\subsection{导出合并后的模型}

\begin{itemize}
\item 为什么要合并:因为LoRA只是通过\textbf{低秩矩阵}调整原始模型的部分权重,而\textbf{不直接修改原模型的权重}。合并步骤将LoRA权重与原始模型权重融合生成一个完整的模型
\item 先创建目录,用于存放导出后的模型
\begin{lstlisting}[language=bash]
mkdir -p Models/deepseek-r1-1.5b-merged
\end{lstlisting}
\item 在页面上配置导出路径,导出即可
\end{itemize}

\section{五、模型部署和暴露接口}

\subsection{创建新的conda虚拟环境用于部署模型}

\begin{itemize}
\item 创建环境
\begin{lstlisting}[language=bash]
conda create -n fastApi python=3.10
\end{lstlisting}

\item 激活环境
\begin{lstlisting}[language=bash]
conda activate fastApi
\end{lstlisting}

\item 在该环境中下载部署模型需要的依赖
\begin{lstlisting}[language=bash]
conda install -c conda-forge fastapi uvicorn transformers pytorch
\end{lstlisting}

\begin{lstlisting}[language=bash]
pip install safetensors sentencepiece protobuf
\end{lstlisting}
\end{itemize}

\subsection{通过FastAPI部署模型并暴露HTTP接口}

\begin{itemize}
\item 创建App文件夹
\begin{lstlisting}[language=bash]
mkdir App
\end{lstlisting}

\item 创建\texttt{main.py}文件,作为启动应用的入口
\begin{lstlisting}[language=bash]
touch main.py
\end{lstlisting}

\item 修改\texttt{main.py}文件并保存
\begin{lstlisting}[language=python]
from fastapi import FastAPI
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

app = FastAPI()

# 模型路径
model_path = "/root/autodl-tmp/Models/deepseek-r1-1.5b-merged"

# 加载tokenizer(分词器)
tokenizer = AutoTokenizer.from_pretrained(model_path)

# 加载模型并移动到可用设备(GPU/CPU)
device = "cuda" if torch.cuda.is_available() else "cpu"
model = AutoModelForCausalLM.from_pretrained(model_path).to(device)

@app.get("/generate")
async def generate_text(prompt: str):
        # 使用tokenizer编码输入的prompt
        inputs = tokenizer(prompt, return_tensors="pt").to(device)
        
        # 使用模型生成文本
        outputs = model.generate(inputs["input_ids"], max_length=150)
        
        # 解码生成的输出
        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        return {"generated_text": generated_text}
\end{lstlisting}

\item 进入包含\texttt{main.py}文件的目录,然后运行以下命令来启动FastAPI应用
\begin{lstlisting}[language=bash]
uvicorn main:app --reload --host 0.0.0.0
\end{lstlisting}

\begin{itemize}
\item \texttt{main}是Python文件名(要注意不包含\texttt{.py}扩展名)
\item \texttt{app}是FastAPI实例的变量名(代码中\texttt{app = FastAPI()})
\item \texttt{--reload}使代码更改后可以自动重载,适用于开发环境
\item \texttt{host 0.0.0.0}:将FastAPI应用绑定到所有可用的网络接口,这样我们的本机就可以通过内网穿透访问该服务
\end{itemize}

\item 配置端口转发,使得本机可以访问该服务,参考:\href{https://www.autodl.com/docs/ssh_proxy/}{SSH隧道}

\item 浏览器输入以下url,测试服务是否启动成功
\begin{verbatim}
http://localhost:8000/docs
\end{verbatim}

\item 或者你也可以通过postMan来测试
\begin{verbatim}
http://localhost:8000/generate?prompt=你是谁?
\end{verbatim}
\end{itemize}

\section{六、web后端调用}

\subsection{pom.xml导入依赖}

\begin{lstlisting}[language=xml]
<dependency>  
        <groupId>org.apache.httpcomponents.client5</groupId>  
        <artifactId>httpclient5</artifactId>  
        <version>5.2.1</version>  
</dependency>
\end{lstlisting}

\subsection{自定义方法发送并处理HTTP请求,实现对话功能}

\begin{lstlisting}[language=java]
@Service  
public class ChatServiceImpl implements ChatService {  
            
        @Autowired  
        private RestTemplate restTemplate;  
        @Autowired  
        private AiServiceConfig aiServiceConfig;  
    
        @Override  
        public String callAiForOneReply(String prompt) {  
                // 获取基础URL http://localhost:8000  
                String baseUrl = aiServiceConfig.getBaseUrl();  
                // 构建完整的请求URL http://localhost:8000/generate?prompt=XXX  
                String url = String.format("%s/generate?prompt=%s", baseUrl, prompt);  
                // 发送GET请求并获取响应  
                GenerateResponse response = restTemplate.getForObject(url, GenerateResponse.class);  
                // 从响应中取出generated_text字段值返回  
                return response != null ? response.getGenerated_text() : "";  
        }  
}
\end{lstlisting}

\subsection{本机启动Demo前后端工程,测试对话效果}

\subsubsection{启动前端工程}

\begin{itemize}
\item 前端项目地址:
\begin{verbatim}
https://github.com/huangyf2013320506/magic_conch_frontend.git
\end{verbatim}

\item 执行:
\begin{lstlisting}[language=bash]
npm install
\end{lstlisting}

\begin{lstlisting}[language=bash]
npm run dev
\end{lstlisting}
\end{itemize}

\subsubsection{启动后端工程}

\begin{itemize}
\item 后端项目地址:
\begin{verbatim}
https://github.com/huangyf2013320506/magic_conch_backend.git
\end{verbatim}

\item 执行:
\begin{lstlisting}[language=bash]
mvn clean install
\end{lstlisting}

\item 在\texttt{MagicConchBackendApplication.java}类中启动
\end{itemize}

\subsection{FastAPI支持自定义多种请求响应格式,可自行探索}

\subsection{如何开放服务端口到公网}

AutoDL当前仅支持个人用户通过端口转发在本地访问服务,如需开放服务端口到公网一般需要企业认证,请参考:\href{https://www.autodl.com/docs/port/}{开放端口}

\subsection{企业部署还需考虑高并发、高可用、安全机制等问题}

\end{document}
